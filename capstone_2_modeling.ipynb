{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35cc0851-bacb-4f97-81f4-ff129c817c39",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The goal of this project is to determine the factors that directly contribute to student success for this online program. Previous EDA showed that the distributions of numeric data did not differ between passing and failing students. Since there are both continuous and categorical values to predict the student outcome, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da58ad09-d042-4816-9afd-dba404e2dca4",
   "metadata": {},
   "source": [
    "## Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40eac41-1ac0-4f46-a2d4-9d991567d2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import itertools\n",
    "from subprocess import call\n",
    "from io import StringIO\n",
    "\n",
    "# Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "from scipy import stats, optimize, spatial\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.colors\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import Image\n",
    "import graphviz\n",
    "import pydotplus\n",
    "\n",
    "# Modeling\n",
    "from sklearn import datasets, svm, decomposition\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesRegressor, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz\n",
    "from sklearn.cluster import KMeans, AffinityPropagation, SpectralClustering, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split, KFold, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, auc\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import precision_score, recall_score, classification_report\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, log_loss\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36141cd2-40a0-427e-b2a9-bb173fba93c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Loading statistical functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82877a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecdf(data):\n",
    "    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n",
    "    n = len(data)\n",
    "    x = np.sort(data)\n",
    "    y = np.arange(1, n + 1) / n\n",
    "    return x, y\n",
    "\n",
    "def pearson_r(x , y):\n",
    "    \"\"\"Compute Pearson correlation coefficient between two arrays.\"\"\"\n",
    "    corr_mat = np.corrcoef(x, y)\n",
    "    return corr_mat[0, 1]\n",
    "\n",
    "def bootstrap_replicate_1d(data, func):\n",
    "    \"\"\"Generate bootstrap replicate of 1D data.\"\"\"\n",
    "    bs_sample = np.random.choice(data, len(data))\n",
    "    return func(bs_sample)\n",
    "\n",
    "def draw_bs_reps(data, func, size = 1):\n",
    "    \"\"\"Draw bootstrap replicates.\"\"\"\n",
    "    bs_replicates = np.empty(size)\n",
    "    for i in range(size):\n",
    "        bs_replicates[i] = bootstrap_replicate_1d(data, func)\n",
    "    return bs_replicates\n",
    "\n",
    "def draw_bs_pairs_linreg(x, y, size = 1):\n",
    "    \"\"\"perform pairs bootstrap for linear regression\"\"\"\n",
    "    inds = np.arange(len(x))\n",
    "    bs_slope_reps = np.empty(size)\n",
    "    bs_intercept_reps = np.empty(size)\n",
    "    for i in range(size):\n",
    "        bs_inds = np.random.choice(inds, size = len(inds))\n",
    "        bs_x, bs_y = x[bs_inds], y[bs_inds]\n",
    "        bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, 1)\n",
    "    return bs_slope_reps, bs_intercept_reps\n",
    "\n",
    "def draw_bs_pairs(x, y, func, size = 1):\n",
    "    \"\"\"Perform pairs bootstrap for a single statistic.\"\"\"\n",
    "    inds = np.arange(len(x))\n",
    "    bs_replicates = np.empty(size)\n",
    "    for i in range(size):\n",
    "        bs_inds = np.random.choice(inds, size = len(inds))\n",
    "        bs_x, bs_y = x[bs_inds], y[bs_inds]\n",
    "        bs_replicates[i] = func(bs_x, bs_y)\n",
    "    return bs_replicates\n",
    "\n",
    "def permutation_sample(data1, data2):\n",
    "    \"\"\"Generate a permutation sample from two data sets.\"\"\"\n",
    "    data = np.concatenate((data1, data2))\n",
    "    permuted_data = np.random.permutation(data)\n",
    "    perm_sample_1 = permuted_data[:len(data1)]\n",
    "    perm_sample_2 = permuted_data[len(data1):]\n",
    "    return perm_sample_1, perm_sample_2\n",
    "\n",
    "def draw_perm_reps(data_1, data_2, func, size=1):\n",
    "    \"\"\"Generate multiple permutation replicates.\"\"\"\n",
    "    perm_replicates = np.empty(size)\n",
    "    for i in range(size):\n",
    "        perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2)\n",
    "        perm_replicates[i] = func(perm_sample_1, perm_sample_2)\n",
    "    return perm_replicates\n",
    "\n",
    "def diff_of_means(data_1, data_2):\n",
    "    \"\"\"Difference in means of two arrays.\"\"\"\n",
    "    diff = np.mean(data_1) - np.mean(data_2)\n",
    "    return diff\n",
    "\n",
    "def diff_frac(data_A, data_b):\n",
    "    frac_A = np.sum(data_A) / len(data_A)\n",
    "    frac_B = np.sum(data_B) / len(data_B)\n",
    "    return frac_B - frac_A\n",
    "\n",
    "def rmse(pred, obs):\n",
    "    return np.sqrt(((pred - obs) ** 2).mean())\n",
    "\n",
    "def mse(pred, obs):\n",
    "    return ((pred - obs) ** 2).mean()\n",
    "\n",
    "def bon_correct(alpha, n):\n",
    "    return (alpha/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4be534-1802-42e6-8b81-6b833906c39b",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223ee967-c5ff-48a6-af7e-5ceaa8caacd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('capstone_2_modeling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e80d90-1d63-4faa-9a33-3fd1abba902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = 'Unnamed: 0', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2552e0-9a72-4423-b00d-2957fde5b096",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d040342b-6143-4612-9875-227ffc5b9076",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bcfe53-e2f9-406d-8ec9-12ee63047914",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283c4429-710e-492e-a71d-ed8244240ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b57b4d-e137-46f1-9da9-095d158157b1",
   "metadata": {},
   "source": [
    "# Splitting the Data\n",
    "\n",
    "Now that the data has been encoded, it can now be split into its respective x and y variables. The 'final_result' feature was left untouched since it was the response variable to all the other features. For x data, 'id_student' will be dropped since it serves as more of a categorical variable.\n",
    "\n",
    "Since the data points were originally ordered by assessment and student, I will need to shuffle the train_test to make sure students at the bottom of the df do not unnecessarily get cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd1d1f7-c3ee-4c5a-9d1a-22b63b3cf38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(columns = ['id_student', 'final_result'])\n",
    "y = df[['final_result']]\n",
    "\n",
    "features = x.columns.tolist()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 6022, stratify = y, test_size = 0.25, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a8233e-4636-401c-b3ef-b73e1668caab",
   "metadata": {},
   "source": [
    "# Scaling\n",
    "\n",
    "Despite encoding, a few columns are still beyond a comparable scope of eachother. A StandardScaler will be applied to ensure that all values are comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b55c86-0c42-468b-91db-38cdb655fd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler_model = scaler.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dfdacb-a29d-48de-8f69-ef1426cc6c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scale = scaler_model.transform(x_train)\n",
    "x_test_scale = scaler_model.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118bf0c5-e449-4c4b-bd22-71ba1b478f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_k = StratifiedKFold(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201fcc35-8559-469e-8640-ba8e109ef1be",
   "metadata": {},
   "source": [
    "# Simple Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550467b2-41ac-4bfd-94ce-4d729edcc2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_params = {\n",
    "    'criterion': ['entropy', 'gini'],\n",
    "    'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'min_samples_split': np.arange(2, 40)\n",
    "}\n",
    "\n",
    "cv_dt = DecisionTreeClassifier(random_state = 6022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6f753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 20000, num = 10)],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "    'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'min_samples_split': np.arange(2, 40)\n",
    "}\n",
    "\n",
    "cv_dt = DecisionTreeClassifier(random_state = 6022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735cd5f2-0d53-4aa1-865b-28f683841cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_rand = RandomizedSearchCV(cv_dt, dt_params, cv = cv_k, n_iter = 200, random_state = 6022)\n",
    "dt_rand_cv = dt_rand.fit(x_train_scale, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a891466-7410-4e44-90b5-7ada9f5a9d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Score:\" + str(dt_rand_cv.best_score_))\n",
    "print(\"Best Parameters: \" + str(dt_rand_cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07064995-d04e-4b9f-9cf7-e467460ae9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(**dt_rand_cv.best_params_, class_weight = 'balanced', random_state = 6022)\n",
    "dt_model = dt.fit(x_train_scale, y_train)\n",
    "dt_pred = dt_model.predict(x_test_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28982f07-d0e5-4fa3-b51e-2387e7da9811",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dt_cv_scores_test = cross_val_score(dt_model, x_test_scale, y_test, cv = cv_k, scoring = 'roc_auc')\n",
    "dt_cv_scores_train = cross_val_score(dt_model, x_train_scale, y_train, cv = cv_k, scoring = 'roc_auc')\n",
    "print(f'Training CV Score: {dt_cv_scores_train.mean()} +- {dt_cv_scores_train.std()}')\n",
    "print(f'Testing CV Score: {dt_cv_scores_test.mean()} +- {dt_cv_scores_test.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4907fbe5-c223-4a5d-8166-f512540af88a",
   "metadata": {},
   "source": [
    "Because of the heavy class imbalance, the models cannot be compared using cross-validation scores. The metrics obtained from the confusion matrix will be how the final model is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505b1ca8-f4f7-41f3-a25a-0603891d8473",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_confusion = confusion_matrix(y_test, dt_pred, labels = dt_model.classes_)\n",
    "dt_confusion_matrix = ConfusionMatrixDisplay(confusion_matrix = dt_confusion, display_labels = dt_model.classes_)\n",
    "dt_confusion_matrix.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f35880e-4054-4d88-9964-beb2ae92fa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, dt_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b72146b-baab-4d5a-995a-34b63afe1b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_imp = dt_model.feature_importances_\n",
    "df_feat_imp_df = pd.DataFrame({'feature': features, 'importance': dt_imp})\n",
    "dt_sorted = df_feat_imp_df.sort_values(by = 'importance', ascending = False)\n",
    "dt_feat_plot = sns.catplot(data = dt_sorted.head(20), kind = 'bar', x = 'feature', y = 'importance', height = 5, aspect = 2)\n",
    "dt_feat_plot = plt.xticks(rotation = 90)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
